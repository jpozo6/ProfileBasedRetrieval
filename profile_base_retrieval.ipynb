{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570bb0e2-5354-4551-83ba-06775149340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "#import gensim\n",
    "#from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7837561a-4454-4bfc-875a-6c534dac2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\.conda\\envs\\PBE_22\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2199: FutureWarning: The parsing of 'now' in pd.to_datetime without `utc=True` is deprecated. In a future version, this will match Timestamp('now') and Timestamp.now()\n",
      "  result, tz_parsed = tslib.array_to_datetime(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "\n",
    "    targets = pd.DataFrame( newsgroups_train.target_names)\n",
    "    targets.columns=['title']\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    out['date'] = pd.to_datetime('now')\n",
    "    out.to_csv('20_newsgroup.csv')\n",
    "\n",
    "twenty_newsgroup_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077005d8-9a8e-412a-9ba7-4acb5a8f4eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2022-04-07 15:08:27.454395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the blood of the lamb.\\n\\nThis will be a hard ...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2022-04-07 15:08:27.454395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>930418\\n\\nDo what thou wilt shall be the whole...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2022-04-07 15:08:27.454395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>\\n\\nNo.  Zeno's paradox is resolved by showing...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2022-04-07 15:08:27.454395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>\\nDefinitely, J.R. \"Bob\" Dobbs, numero uno, to...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2022-04-07 15:08:27.454395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target  \\\n",
       "9   \\nIf a Christian means someone who believes in...      19   \n",
       "10  the blood of the lamb.\\n\\nThis will be a hard ...      19   \n",
       "12  930418\\n\\nDo what thou wilt shall be the whole...      19   \n",
       "40  \\n\\nNo.  Zeno's paradox is resolved by showing...      19   \n",
       "58  \\nDefinitely, J.R. \"Bob\" Dobbs, numero uno, to...      19   \n",
       "\n",
       "                 title                        date  \n",
       "9   talk.religion.misc  2022-04-07 15:08:27.454395  \n",
       "10  talk.religion.misc  2022-04-07 15:08:27.454395  \n",
       "12  talk.religion.misc  2022-04-07 15:08:27.454395  \n",
       "40  talk.religion.misc  2022-04-07 15:08:27.454395  \n",
       "58  talk.religion.misc  2022-04-07 15:08:27.454395  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('20_newsgroup.csv', index_col = 0)\n",
    "# Select only 5 classes\n",
    "class_mask = data[\"target\"].isin([7,9,13,18,19])\n",
    "data = data[class_mask]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d54a3b-dd2c-41d2-aecb-5f0dbddf6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "df_train, df_test = train_test_split(data,test_size=0.1,shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4972dc5-4fa6-46a9-b124-7c6b67d3c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38160674-1a56-4f79-8dbb-86c990f7cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06689a0-78d3-4ae9-b644-545eee7f8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head()\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacc5b24-f11d-4561-80ea-b04bd7fc5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train[\"clean_text\"],df_train[\"title\"],test_size=0.2,shuffle=True, stratify=df_train[\"title\"], random_state=2)\n",
    "X_test = df_test['clean_text']\n",
    "y_test = df_test['title']\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]\n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4272d8-565e-4213-84bf-de3793ec88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4c27e8-641b-4b15-9e5a-cd299a4fc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         rec.autos       0.91      0.96      0.93       100\n",
      "rec.sport.baseball       0.90      0.93      0.92        88\n",
      "           sci.med       0.95      0.94      0.94        95\n",
      "talk.politics.misc       0.87      0.88      0.88        86\n",
      "talk.religion.misc       0.84      0.70      0.77        54\n",
      "\n",
      "          accuracy                           0.90       423\n",
      "         macro avg       0.89      0.88      0.89       423\n",
      "      weighted avg       0.90      0.90      0.90       423\n",
      "\n",
      "Confusion Matrix:\n",
      " [[96  1  0  2  1]\n",
      " [ 2 82  1  2  1]\n",
      " [ 3  1 89  1  1]\n",
      " [ 1  5  0 76  4]\n",
      " [ 4  2  4  6 38]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'balanced_accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m nn_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_predict)\n\u001b[0;32m     10\u001b[0m nn_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_predict, average \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m nn_bacc \u001b[38;5;241m=\u001b[39m \u001b[43mbalanced_accuracy_score\u001b[49m(y_test, y_predict)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy of Neural Network(tf-idf) is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest F1-score weighted of Neural Network(tf-idf) is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnn_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'balanced_accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "nn_tfidf = MLPClassifier()\n",
    "nn_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "#Predict y value for test dataset\n",
    "y_predict = nn_tfidf.predict(X_test_vectors_tfidf)\n",
    "#y_prob = dt_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test, y_predict))\n",
    "\n",
    "nn_acc = accuracy_score(y_test, y_predict)\n",
    "nn_f1 = f1_score(y_test, y_predict, average ='weighted')\n",
    "\n",
    "print(f\"Test Accuracy of Neural Network(tf-idf) is {nn_acc} \\n\")\n",
    "print(f\"Test F1-score weighted of Neural Network(tf-idf) is {nn_f1} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f553d3a-8c40-4f71-ad8d-5f72eccaa9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x23664 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectors_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40568a0d-94b0-4383-bcae-07d00065af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce69e757-6bbf-4d76-8774-1d59dd08dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = [\"autos\", \"baseball\", \"medicine\", \"politics\", \"religion\"]\n",
    "\n",
    "def generate_users(n_user, interests= [\"autos\", \"baseball\", \"medicine\", \"politics\", \"religion\"]):\n",
    "    users = {}\n",
    "    for user in range(n_user):\n",
    "        n_interest = random.randint(1, 3)\n",
    "        user_interest = []\n",
    "        i = random.sample(range(0,4), n_interest)\n",
    "        user_interest.append([interests[e] for e in i])\n",
    "            \n",
    "        users[user] = user_interest\n",
    "    return users\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9621dfa-5de6-4b82-814d-8635b14ad155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [['politics']], 1: [['medicine']], 2: [['baseball']], 3: [['baseball', 'medicine', 'politics']], 4: [['medicine']], 5: [['politics', 'baseball', 'autos']], 6: [['medicine', 'autos']], 7: [['autos', 'politics', 'baseball']], 8: [['baseball', 'politics']], 9: [['autos', 'politics']]}\n"
     ]
    }
   ],
   "source": [
    "users = generate_users(10)\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2c2639a-cbef-4e10-9d06-5cacbff5343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "med\n"
     ]
    }
   ],
   "source": [
    "pred = nn_tfidf.predict(X_test_vectors_tfidf[1])\n",
    "print(pred[0].split('.')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "310f0cd2-8754-43a8-8d38-4666944bc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assigne_text(users, pred):\n",
    "    if 'med' in pred:\n",
    "        text = 'medicine'\n",
    "    elif 'autos' in pred:\n",
    "        text = 'autos'\n",
    "    elif 'politics' in pred:\n",
    "        text = 'politics'\n",
    "    elif 'religion' in pred:\n",
    "        text = 'religion'\n",
    "    elif 'baseball':\n",
    "        text = 'baseball'\n",
    "    assigned = []\n",
    "    for user, interest in users.items():\n",
    "        if text in interest[0]:\n",
    "            assigned.append(user)\n",
    "            \n",
    "    return assigned, text.upper()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd8c9711-aa50-4a22-ab16-22bfa3d61792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is assigned to user 1. It is a medicine text.\n",
      "This text is assigned to user 3. It is a medicine text.\n",
      "This text is assigned to user 4. It is a medicine text.\n",
      "This text is assigned to user 6. It is a medicine text.\n"
     ]
    }
   ],
   "source": [
    "assigned, text_topic = assigne_text(users, pred[0])\n",
    "for user in assigned:\n",
    "    print(\"This text is assigned to user {}. It is a {} text.\".format(user, assigned_text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a45cfe-c3f5-47d7-9598-e61dacbc0940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
